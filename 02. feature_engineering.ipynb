{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create price feature based on POS sales and POS dollars\n",
    "2. Impute nulls in the dataset\n",
    "3. Vectorization\n",
    "4. Moving Average Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ds_challenge_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Hash_Trans_ID', 'SKU_ID', 'Transaction_Date', 'Transaction_Year',\n",
       "       'Transaction_Month', 'Transaction_Day', 'Transaction_Hour',\n",
       "       'Transaction_Min', 'Transaction_Sec', 'Transaction_Day_of_Week',\n",
       "       'Transaction_Week', 'Transaction_Day_of_Year', 'holiday_nm',\n",
       "       'Transaction_Type', 'Transaction_Sales_Type', 'POS_Sales',\n",
       "       'POS_UnitsSold', 'Department_ID', 'Class_ID', 'SubClass_ID',\n",
       "       'SubClass_Desc', 'Product_Weight_Grams', 'Product_Height_CM',\n",
       "       'Product_Length_CM', 'Product_Width_CM', 'Product_Volumn_Metric',\n",
       "       'PRO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'] = df['POS_Sales']/df['POS_UnitsSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create SKU level price features seperate for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main assumption is that we know the price one year before\n",
    "sku_df = df.groupby(['SKU_ID','Transaction_Year']).agg({'Price':np.mean}).reset_index()\n",
    "sku_df['SKU_AVG_Price'] = sku_df['Price']\n",
    "sku_df = sku_df[['SKU_ID','Transaction_Year','SKU_AVG_Price']]\n",
    "\n",
    "sku_df2 = df.groupby(['SKU_ID','Transaction_Year']).agg({'Price':np.min}).reset_index()\n",
    "sku_df2['SKU_MIN_Price'] = sku_df2['Price']\n",
    "sku_df2 = sku_df2[['SKU_ID','Transaction_Year','SKU_MIN_Price']]\n",
    "\n",
    "sku_df3 = df.groupby(['SKU_ID','Transaction_Year']).agg({'Price':np.max}).reset_index()\n",
    "sku_df3['SKU_MAX_Price'] = sku_df3['Price']\n",
    "sku_df3 = sku_df3[['SKU_ID','Transaction_Year','SKU_MAX_Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_df = sku_df.merge(sku_df2, on=['SKU_ID','Transaction_Year']).merge(sku_df3, on=['SKU_ID','Transaction_Year'])\n",
    "df = df.merge(sku_df, on=['SKU_ID','Transaction_Year'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is better to be aggregated to transaction level instead of transaction-sku level\n",
    "At transaction level vectors that are built based on product attributes are immune to problems like similar SKUs and product cannibalization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Nulls to start Vectorization\n",
    "#df.fillna('none', inplace = True)\n",
    "df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Txn Type, Sales Type and Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction_Type\n",
       "Commercial Line Sale    0.613808\n",
       "Front line - Sales      0.419146\n",
       "Front line -Self Chk    0.309424\n",
       "Garden Sales            0.230607\n",
       "Pro Desk - Refund       0.737839\n",
       "Pro Desk - Sales        0.590201\n",
       "Returns desk - Refnd    0.406545\n",
       "Sp. Srvc. Dsk  VPOS     0.475993\n",
       "Sp. Srvc. Dsk Refnd     0.407097\n",
       "Tool Rental -Refund     0.651173\n",
       "Tools - Refund          0.674731\n",
       "Name: PRO, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Transaction_Type')['PRO'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[((df['Transaction_Type']=='Pro Desk - Refund') & (df['PRO']==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holiday_nm\n",
       "0                        0.525168\n",
       "Boxing Day               0.328440\n",
       "Boxing Day (Observed)    0.315217\n",
       "Canada Day               0.376829\n",
       "Civic Holiday            0.410287\n",
       "Family Day               0.276243\n",
       "Good Friday              0.328205\n",
       "Labour Day               0.377809\n",
       "New Year's Day           0.349398\n",
       "Thanksgiving             0.347150\n",
       "Victoria Day             0.350222\n",
       "Name: PRO, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('holiday_nm')['PRO'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction_Sales_Type\n",
       "Gift Card Sales         0.238806\n",
       "Returns                 0.380371\n",
       "Sales                   0.490536\n",
       "Special Order Return    0.562130\n",
       "Special Order Sales     0.712379\n",
       "Name: PRO, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Transaction_Sales_Type')['PRO'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Department_ID\n",
       "11    0.463248\n",
       "13    0.238806\n",
       "16    0.603203\n",
       "17    0.225131\n",
       "21    0.562079\n",
       "22    0.590631\n",
       "23    0.456892\n",
       "24    0.529044\n",
       "25    0.555719\n",
       "26    0.553793\n",
       "27    0.469824\n",
       "28    0.413927\n",
       "29    0.553191\n",
       "30    0.569670\n",
       "59    0.432815\n",
       "78    0.782479\n",
       "Name: PRO, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Department_ID')['PRO'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering SubClass Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SubClass_Desc_Clean'] = df['SubClass_Desc'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SubClass_Desc_Clean']= df['SubClass_Desc_Clean'].apply(lambda x: x.lower().replace('XX','').replace('xx','')  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SubClass_Desc_Clean']= df['SubClass_Desc_Clean']\\\n",
    "        .apply(lambda x: x.replace('&','').replace('-','').replace('/','').replace('$','')\\\n",
    "                .replace('%','').replace(' ','').replace(\"'\",'').replace(\".\",'').strip()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", analyzer='char', ngram_range=(3, 5), max_df=1.0, min_df=1, max_features=None)\n",
    "X = vectorizer.fit_transform(df['SubClass_Desc_Clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=10, n_init=1, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, n_jobs=-1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SubClass_Desc_Cluster'] = df['SubClass_Desc_Clean'].apply(lambda x: model.predict(vectorizer.transform([x])).astype('int')[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubClass_Desc_Cluster\n",
       "0      1205\n",
       "1      4707\n",
       "2      2495\n",
       "3       244\n",
       "4      2770\n",
       "5      9953\n",
       "6      1578\n",
       "7      7947\n",
       "8      2150\n",
       "9    288798\n",
       "Name: Hash_Trans_ID, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['SubClass_Desc_Cluster']).count()['Hash_Trans_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubClass_Desc_Cluster\n",
       "0    0.527801\n",
       "1    0.591884\n",
       "2    0.560321\n",
       "3    0.520492\n",
       "4    0.653791\n",
       "5    0.462976\n",
       "6    0.582383\n",
       "7    0.523216\n",
       "8    0.611628\n",
       "9    0.519817\n",
       "Name: PRO, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['SubClass_Desc_Cluster'])['PRO'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data=df, columns = ['Transaction_Type','Transaction_Sales_Type','Department_ID','SubClass_Desc_Cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put back everything into Transaction level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.groupby('Hash_Trans_ID').agg({'SKU_ID':np.size,\n",
    "                                            'Transaction_Date':np.max,\n",
    "                                            'Transaction_Year':np.max, 'Transaction_Month':np.max,\n",
    "                                            'Transaction_Day':np.max, 'Transaction_Hour':np.max, \n",
    "                                            'Transaction_Day_of_Week':np.max,\n",
    "                                            'Transaction_Week':np.max, \n",
    "                                            'Transaction_Day_of_Year':np.max, \n",
    "                                            'holiday_nm':np.max,\n",
    "                                            'PRO':np.max}).reset_index()\n",
    "\n",
    "df_final = df_final.rename(columns={'SKU_ID': 'SKU_ID_Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sum/Min/Max/Std Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df.groupby('Hash_Trans_ID').agg({\n",
    "                                'POS_Sales':np.sum,\n",
    "                                'POS_UnitsSold':np.sum, \n",
    "                                'Product_Length_CM':np.sum,\n",
    "                                'Product_Height_CM':np.sum,\n",
    "                                'Product_Width_CM':np.sum,\n",
    "                                'Product_Weight_Grams':np.sum, \n",
    "                                'Product_Volumn_Metric':np.sum,\n",
    "                                'Price':np.sum, \n",
    "                                'SKU_AVG_Price':np.sum,\n",
    "                                'SKU_MIN_Price':np.sum, \n",
    "                                'SKU_MAX_Price':np.sum, \n",
    "                                'Transaction_Type_1':np.sum,\n",
    "                                'Transaction_Type_2':np.sum,\n",
    "                                'Transaction_Type_3':np.sum,\n",
    "                                'Transaction_Type_4':np.sum,\n",
    "                                'Transaction_Type_5':np.sum,\n",
    "                                'Transaction_Type_6':np.sum,\n",
    "                                'Transaction_Type_7':np.sum,\n",
    "                                'Transaction_Type_8':np.sum,\n",
    "                                'Transaction_Type_9':np.sum,\n",
    "                                'Transaction_Type_10':np.sum,\n",
    "                                'Transaction_Type_11':np.sum,\n",
    "                                'Transaction_Sales_Type_12':np.sum,\n",
    "                                'Transaction_Sales_Type_13':np.sum, \n",
    "                                'Transaction_Sales_Type_14':np.sum,\n",
    "                                'Transaction_Sales_Type_15':np.sum,\n",
    "                                'Transaction_Sales_Type_16':np.sum,\n",
    "                                'Department_ID_13':np.sum, 'Department_ID_16':np.sum, 'Department_ID_17':np.sum,\n",
    "                                'Department_ID_21':np.sum, 'Department_ID_22':np.sum, 'Department_ID_23':np.sum,\n",
    "                                'Department_ID_24':np.sum, 'Department_ID_25':np.sum, 'Department_ID_26':np.sum,\n",
    "                                'Department_ID_27':np.sum, 'Department_ID_28':np.sum, 'Department_ID_29':np.sum,\n",
    "                                'Department_ID_30':np.sum, 'Department_ID_59':np.sum, 'Department_ID_78':np.sum,\n",
    "                                'SubClass_Desc_Cluster_0':np.sum, 'SubClass_Desc_Cluster_1':np.sum,\n",
    "                                'SubClass_Desc_Cluster_2':np.sum, 'SubClass_Desc_Cluster_3':np.sum,\n",
    "                                'SubClass_Desc_Cluster_4':np.sum, 'SubClass_Desc_Cluster_5':np.sum,\n",
    "                                'SubClass_Desc_Cluster_6':np.sum, 'SubClass_Desc_Cluster_7':np.sum,\n",
    "                                'SubClass_Desc_Cluster_8':np.sum, 'SubClass_Desc_Cluster_9':np.sum\n",
    "                                }).reset_index()\n",
    "\n",
    "df_sum = df_sum.rename(columns={'POS_Sales': 'POS_Sales_Sum',\n",
    "                                    'POS_UnitsSold': 'POS_UnitsSold_Sum', 'Product_Length_CM': 'Product_Lengthe_Sum',\n",
    "                                    'Product_Width_CM': 'Product_Widthe_Sum', 'Product_Weight_Grams': 'Product_Weighte_Sum',\n",
    "                                    'Price': 'Price_Sum',  \n",
    "                                    'SKU_AVG_Price':'SKU_AVG_Price_Sum',\n",
    "                                    'SKU_MIN_Price':'SKU_MIN_Price_Sum', \n",
    "                                    'SKU_MAX_Price':'SKU_MAX_Price_Sum',\n",
    "                                    'Product_Height_CM': 'Product_Heighte_Sum',\n",
    "                                    'Product_Volumn_Metric': 'Product_Volumee_Sum', 'Product_Height_CM': 'Product_Heighte_Sum'})\n",
    "\n",
    "df_final = df_final.merge(df_sum, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.groupby('Hash_Trans_ID').agg({\n",
    "                                'Price':np.mean, \n",
    "                                'SKU_AVG_Price':np.mean,\n",
    "                                'SKU_MIN_Price':np.mean, \n",
    "                                'SKU_MAX_Price':np.mean, \n",
    "                                'POS_Sales':np.mean,\n",
    "                                'POS_UnitsSold':np.mean, \n",
    "                                'Product_Length_CM':np.mean, \n",
    "                                'Product_Width_CM':np.mean,\n",
    "                                'Product_Weight_Grams':np.mean, \n",
    "                                'Product_Height_CM':np.mean,\n",
    "                                'Product_Volumn_Metric':np.mean}).reset_index()\n",
    "\n",
    "df_mean = df_mean.rename(columns={'Price':'Price_Avg', \n",
    "                                'SKU_AVG_Price':'SKU_AVG_Price_Avg',\n",
    "                                'SKU_MIN_Price':'SKU_MIN_Price_Avg', \n",
    "                                'SKU_MAX_Price':'SKU_MAX_Price_Avg', \n",
    "                                'POS_Sales':'POS_Sales_Avg',\n",
    "                                'POS_UnitsSold':'POS_UnitsSold_Avg', \n",
    "                                'Product_Length_CM':'Product_Length_Avg', \n",
    "                                'Product_Width_CM':'Product_Width_Avg',\n",
    "                                'Product_Weight_Grams':'Product_Weight_Avg', \n",
    "                                'Product_Height_CM':'Product_Height_Avg',\n",
    "                                'Product_Volumn_Metric':'Product_Volumn_Avg'})\n",
    "\n",
    "df_final = df_final.merge(df_mean, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = df.groupby('Hash_Trans_ID').agg({\n",
    "                                'Price':np.max, \n",
    "                                'SKU_AVG_Price':np.max,\n",
    "                                'SKU_MIN_Price':np.max, \n",
    "                                'SKU_MAX_Price':np.max,\n",
    "                                'POS_Sales':np.max,'POS_UnitsSold':np.max, \n",
    "                                'Product_Length_CM':np.max, 'Product_Width_CM':np.max,\n",
    "                                'Product_Weight_Grams':np.max, 'Product_Height_CM':np.max,\n",
    "                                'Product_Volumn_Metric':np.max}).reset_index()\n",
    "\n",
    "df_max = df_max.rename(columns={'POS_Sales': 'POS_Sales_Max', 'POS_UnitsSold': 'POS_UnitsSold_Max',\n",
    "                                    'Product_Length_CM': 'Product_Length_Max', 'Product_Width_CM': 'Product_Width_Max',\n",
    "                                    'Product_Weight_Grams': 'Product_Weight_Max', \n",
    "                                    'Price': 'Price_Max',  \n",
    "                                    'SKU_AVG_Price':'SKU_AVG_Price_Max',\n",
    "                                    'SKU_MIN_Price':'SKU_MIN_Price_Max', \n",
    "                                    'SKU_MAX_Price':'SKU_MAX_Price_Max',\n",
    "                                    'Product_Height_CM': 'Product_Height_Max', 'Product_Volumn_Metric': 'Product_Volume_Max'})\n",
    "\n",
    "df_final = df_final.merge(df_max, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min = df.groupby('Hash_Trans_ID').agg({\n",
    "                                'Price':np.min, \n",
    "                                'SKU_AVG_Price':np.min,\n",
    "                                'SKU_MIN_Price':np.min, \n",
    "                                'SKU_MAX_Price':np.min,\n",
    "                                'POS_Sales':np.min,'POS_UnitsSold':np.min, \n",
    "                                'Product_Length_CM':np.min, 'Product_Width_CM':np.min,\n",
    "                                'Product_Weight_Grams':np.min, 'Product_Height_CM':np.min,\n",
    "                                'Product_Volumn_Metric':np.min}).reset_index()\n",
    "\n",
    "df_min = df_min.rename(columns={'POS_Sales': 'POS_Sales_Min', 'POS_UnitsSold': 'POS_UnitsSold_Min',\n",
    "                                    'Product_Length_CM': 'Product_Length_Min', 'Product_Width_CM': 'Product_Width_Min',\n",
    "                                    'Product_Weight_Grams': 'Product_Weight_Min', \n",
    "                                    'Price': 'Price_Min',  \n",
    "                                    'SKU_AVG_Price':'SKU_AVG_Price_Min',\n",
    "                                    'SKU_MIN_Price':'SKU_MIN_Price_Min', \n",
    "                                    'SKU_MAX_Price':'SKU_MAX_Price_Min',\n",
    "                                    'Product_Height_CM': 'Product_Height_Min', 'Product_Volumn_Metric': 'Product_Volume_Min'})\n",
    "\n",
    "df_final = df_final.merge(df_min, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df.groupby('Hash_Trans_ID').agg({\n",
    "                                'Price':np.std, \n",
    "                                'SKU_AVG_Price':np.std,\n",
    "                                'SKU_MIN_Price':np.std, \n",
    "                                'SKU_MAX_Price':np.std,\n",
    "                                'POS_Sales':np.std,'POS_UnitsSold':np.std, \n",
    "                                'Product_Length_CM':np.std, 'Product_Width_CM':np.std,\n",
    "                                'Product_Weight_Grams':np.std, 'Product_Height_CM':np.std,\n",
    "                                'Product_Volumn_Metric':np.std}).reset_index()\n",
    "\n",
    "df_std = df_std.rename(columns={'POS_Sales': 'POS_Sales_Std', 'POS_UnitsSold': 'POS_UnitsSold_Std',\n",
    "                                    'Product_Length_CM': 'Product_Length_Std', 'Product_Width_CM': 'Product_Width_Std',\n",
    "                                    'Product_Weight_Grams': 'Product_Weight_Std', \n",
    "                                    'Price': 'Price_Std',  \n",
    "                                    'SKU_AVG_Price':'SKU_AVG_Price_Std',\n",
    "                                    'SKU_MIN_Price':'SKU_MIN_Price_Std', \n",
    "                                    'SKU_MAX_Price':'SKU_MAX_Price_Std',\n",
    "                                    'Product_Height_CM': 'Product_Height_Std', 'Product_Volumn_Metric': 'Product_Volume_Std'})\n",
    "\n",
    "df_final = df_final.merge(df_std, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassID and SubClassID Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['basket_row'] = df.sort_values(['SKU_ID'], ascending=[True]) \\\n",
    "             .groupby(['Hash_Trans_ID']) \\\n",
    "             .cumcount() + 1\n",
    "df['basket_row'] = df['basket_row'].apply(lambda x: 11 if x>10 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pd.pivot_table(df, values=['POS_Sales','Class_ID','SubClass_ID'], index='Hash_Trans_ID', \n",
    "                    columns=['basket_row'], aggfunc=np.max).reset_index()\n",
    "pt.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.merge(pt, on=['Hash_Trans_ID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.sort_values(by=['Transaction_Date','Hash_Trans_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Transaction_Type_Commercial Line Sale','Transaction_Type_Front line - Sales','Transaction_Type_Front line -Self Chk',\n",
    "            'Transaction_Type_Garden Sales','Transaction_Type_Pro Desk - Refund','Transaction_Type_Pro Desk - Sales',\n",
    "            'Transaction_Type_Returns desk - Refnd','Transaction_Type_Sp. Srvc. Dsk  VPOS','Transaction_Type_Sp. Srvc. Dsk Refnd',\n",
    "            'Transaction_Type_Tool Rental -Refund','Transaction_Type_Tools - Refund']:\n",
    "    df_cat = df_final[df_final[col]>0]\n",
    "    df_cat = df_cat[['Transaction_Date','PRO']].rolling(50, on='Transaction_Date').mean()\n",
    "    df_cat.fillna(0, inplace = True)\n",
    "    df_cat = df_cat.groupby('Transaction_Date').mean().reset_index()\n",
    "    df_cat = df_cat.rename(columns={'PRO':'Moving_Avg_Txn_'+col})\n",
    "    df_final = df_final.merge(df_cat, on=['Transaction_Date'], how='left')\n",
    "\n",
    "df_final.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the outputs for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('ds_challenge_data_final.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
